---
title: "Machine Learning 1"
author: "Chantal Rabay A1452864"
date: "2/14/2022"
output: pdf_document
---

# Clustering with kmeans() and hclust()

We will begin by making up some data to cluster.
```{r}
tmp <- c(rnorm(30,3), rnorm(30,-3))
x <- cbind(tmp, rev(tmp))
plot(x)
```
## Run kmeans()
```{r}
k <- kmeans(x, centers=2, nstart=20)
print(k)
```
> Q. What size is each CLuster?

```{r}
k$size
```
> Q. Cluster centers

```{r}
k$centers
```

> Q. Membership vector 

```{r}
k$cluster
```
Plot our data with the clustering result.
```{r}
plot(x, col=k$cluster) +
points(k$centers, col="blue", pch=16) +
points(x[50,1], x[50,2], col="darkgreen", pch=16) +
points(x[19,1], x[19,2], col="darkgreen", pch=16)
```
## hclust()

Hierarchical Clustering

```{r}
hc <- hclust(dist(x))
hc
```
Plot method for hclust()
```{r}
plot(hc)
```

## Principal Component Analysis 


## Data Import 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```
> [Q1] How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

>> We can use dim() to return the number of rows and columns or we can use ncol() and nrow() together to return seperately, the number of columns and rows.
```{r}
# Using the dim() function to return the number of rows and columns in the data frame.
dim(x)
```
There are 17 rows and 5 columns.

## Checking your data 

```{r}
#Preview the first 6 rows of the data frame
head(x)
```
We want only 4 columns, the first column x needs to be the rownames/index.
```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

Check the dimensions again 
```{r}
dim(x)
```

Using an alternate way to set the index as the strings in column x
```{r}
x <- read.csv(url, row.names=1)
head(x)
```
> [Q2] Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

>> I preferred the second approach which utilizes the "row.names" argument in "read.csv()". This method is more robust in certain circumstances because if you run the first code block more than once it will continue to move the index to the next column on the right. Additonally, the second method requires less code and is more succint. 

## Spotting Major Differences and Trends

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
> [Q3] Changing what optional argument in the above barplot() function results in the following plot?
>> Changing beside to False

```{r}
#Graphing the same barplot as above, with beside set to False
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

> [Q5] Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```
>> If a given point falls on the diagonal this means that two countries consume the same amount of the type of food that the dot is representing. 

> [Q6] What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
>> From this plot we can see that N. Ireland is more different than the other countries of the UK, however, we cannot necessarily see how it is different.

## PCA to the Rescue!
```{r}
# Use the prcomp() PCA function. This function requires the transpose of our data in this case:
pca <- prcomp( t(x) )
summary(pca)
```

> [Q7] Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
> [Q8] Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))+
text(pca$x[,1], pca$x[,2], labels=colnames(x), col= c("orange", "red" , "blue", "darkgreen"))
```
Use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```
```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

> Digging Deeper (variable loadings)

# Lets focus on PC1 as it accounts for > 90% of variance 
```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

> [Q9] Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

PC2 tells us more about the differences between Scotland and Wales. Soft_drinks and sugars are the two predominate groups here with soft_drinks on the negative, therefore Scotland consumes more soft drinks compared to the other UK countries. However, Wales and possibly N. Ireland consume sugars more than Scotland.

## Bigplots

```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```

## PCA of RNA-seq data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
> [Q10] How many genes and samples are in this data set?

```{r}
#Find the number of rows and columns of the dataframe
dim(rna.data)
```
There are 100 genes (rows) and 10 samples (columns) in this data set.

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```
```{r}
summary(pca)
```

```{r}
plot(pca, main="Quick scree plot")
```

```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```
```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

## Using ggplot
```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```
# add column specific color and labels

```{r}
# Add a 'wt' and 'ko' "condition" column
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```
#Adding titles and captions

```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 example data") +
     theme_bw()
```

> Gene Loadings

```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```


